{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd89de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/Desktop/16-745 OCRL/16745-Optimal-Control-and-RL/HW2_S24-main/Project.toml`\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[33m  ✓ \u001b[39mPlots\n",
      "  1 dependency successfully precompiled in 33 seconds (203 already precompiled)\n",
      "  \u001b[33m1\u001b[39m dependency precompiled but a different version is currently loaded. Restart julia to access the new version\n"
     ]
    }
   ],
   "source": [
    "using Pkg: Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()\n",
    "using LinearAlgebra, Plots\n",
    "import ForwardDiff as FD\n",
    "using Test\n",
    "import Convex as cvx\n",
    "using ECOS: ECOS\n",
    "using Random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d968e4",
   "metadata": {},
   "source": [
    "## Note: \n",
    "\n",
    "Some of the cells below will have multiple outputs (plots and animations), it can be easier to see everything if you do `Cell -> All Output -> Toggle Scrolling`, so that it simply expands the output area to match the size of the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f03c35",
   "metadata": {},
   "source": [
    "## Julia Warnings:\n",
    "\n",
    "1. For a function `foo(x::Vector)` with 1 input argument, it is not neccessary to do `df_dx = FD.jacobian(_x -> foo(_x), x)`. Instead you can just do `df_dx = FD.jacobian(foo, x)`. If you do the first one, it can dramatically slow down your compliation time. \n",
    "\n",
    "2. Do not define functions inside of other functions like this:\n",
    "```julia\n",
    "function foo(x)\n",
    "    # main function foo \n",
    "    \n",
    "    function body(x)\n",
    "        # function inside function (DON'T DO THIS)\n",
    "        return 2*x \n",
    "    end\n",
    "    \n",
    "    return body(x)\n",
    "end\n",
    "```\n",
    "This will also slow down your compilation time dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce77d26",
   "metadata": {},
   "source": [
    "# Q1: Finite-Horizon LQR (55 pts)\n",
    "For this problem we are going to consider a \"double integrator\" for our dynamics model. This system has a state $x \\in \\mathbb{R}^4$, and control $u \\in \\mathbb{R}^2$, where the state describes the 2D position $p$ and velocity $v$ of an object, and the control is the acceleration $a$ of this object.  The state and control are the following:\n",
    "$$\\begin{align} x &= [p_1, p_2, v_1, v_2] \\\\ u &= [a_1, a_2] \\end{align}$$\n",
    "And the continuous time dynamics for this system are the following:\n",
    "$$\\begin{align} \\dot{x} =  \\begin{bmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} x + \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\end{align} u$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976cfe8",
   "metadata": {},
   "source": [
    "## Part A: Discretize the model (5 pts)\n",
    "\n",
    "Use the matrix exponential (`exp` in Julia) to discretize the continuous time model assuming we have a zero-order hold on the control. See [this part of the first recitation](https://youtu.be/97JZi5ztc3c?si=Sxqg_uHnRXoozafM&t=2686) if you're unsure of what to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double integrator dynamics \n",
    "function double_integrator_AB(dt)::Tuple{Matrix, Matrix}\n",
    "\tAc = [0 0 1 0;\n",
    "\t\t0 0 0 1;\n",
    "\t\t0 0 0 0;\n",
    "\t\t0 0 0 0.0]\n",
    "\tBc = [0 0;\n",
    "\t\t0 0;\n",
    "\t\t1 0;\n",
    "\t\t0 1]\n",
    "\tnx, nu = size(Bc)\n",
    "\n",
    "\t# TODO: discretize this linear system using the Matrix Exponential\n",
    "\n",
    "\tA = zeros(nx, nx) # TODO \n",
    "\tB = zeros(nx, nu) # TODO \n",
    "\n",
    "\t@assert size(A) == (nx, nx)\n",
    "\t@assert size(B) == (nx, nu)\n",
    "\n",
    "\treturn A, B\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbfbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"discrete time dynamics\" begin\n",
    "\tdt = 0.1\n",
    "\tA, B = double_integrator_AB(dt)\n",
    "\n",
    "\tx = [1, 2, 3, 4.0]\n",
    "\tu = [-1, -3.0]\n",
    "\n",
    "\t@test isapprox((A * x + B * u), [1.295, 2.385, 2.9, 3.7]; atol = 1e-10)\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6154e8b",
   "metadata": {},
   "source": [
    "## Part B: Finite Horizon LQR via Convex Optimization (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a2439",
   "metadata": {},
   "source": [
    "We are now going to solve the finite horizon LQR problem with convex optimization. As we went over in class, this problem requires $Q \\in S_+$(Q is symmetric positive semi-definite) and $R \\in S_{++}$ (R is symmetric positive definite).  With this, the optimization problem can be stated as the following:\n",
    "$$ \\begin{align} \\min_{x_{1:N},u_{1:N-1}} \\quad & \\sum_{i=1}^{N-1} \\bigg[ \\frac{1}{2} x_i^TQx_i + \\frac{1}{2} u_i^TRu_i \\bigg] + \\frac{1}{2}x_N^TQ_fx_N\\\\ \n",
    " \\text{st} \\quad & x_1 = x_{\\text{IC}} \\\\ \n",
    " & x_{i+1} = A x_i + Bu_i \\quad \\text{for } i = 1,2,\\ldots,N-1 \n",
    " \\end{align}$$\n",
    " This problem is a convex optimization problem since the cost function is a convex quadratic and the constraints are all linear equality constraints. We will setup and solve this exact problem using the `Convex.jl` modeling package. (See 2/16 Recitation video for help with this package. [Notebook is here](https://github.com/Optimal-Control-16-745/recitations/blob/main/2_17_recitation/Convex.jl_tutorial.ipynb).)  Your job in the block below is to fill out a function `Xcvx,Ucvx = convex_trajopt(A,B,Q,R,Qf,N,x_ic)`, where you will form and solve the above optimization problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a5cfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convex_trajopt (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utilities for converting to and from vector of vectors <-> matrix \n",
    "function mat_from_vec(X::Vector{Vector{Float64}})::Matrix\n",
    "\t# convert a vector of vectors to a matrix \n",
    "\tXm = hcat(X...)\n",
    "\treturn Xm\n",
    "end\n",
    "function vec_from_mat(Xm::Matrix)::Vector{Vector{Float64}}\n",
    "\t# convert a matrix into a vector of vectors \n",
    "\tX = [Xm[:, i] for i ∈ 1:size(Xm, 2)]\n",
    "\treturn X\n",
    "end\n",
    "\"\"\"\n",
    "X,U = convex_trajopt(A,B,Q,R,Qf,N,x_ic; verbose = false)\n",
    "\n",
    "This function takes in a dynamics model x_{k+1} = A*x_k + B*u_k\n",
    "and LQR cost Q,R,Qf, with a horizon size N, and initial condition \n",
    "x_ic, and returns the optimal X and U's from the above optimization \n",
    "problem. You should use the `vec_from_mat` function to convert the \n",
    "solution matrices from cvx into vectors of vectors (vec_from_mat(X.value))\n",
    "\"\"\"\n",
    "function convex_trajopt(A::Matrix,      # A matrix \n",
    "\tB::Matrix,      # B matrix \n",
    "\tQ::Matrix,      # cost weight \n",
    "\tR::Matrix,      # cost weight \n",
    "\tQf::Matrix,     # term cost weight \n",
    "\tN::Int64,       # horizon size \n",
    "\tx_ic::Vector;   # initial condition\n",
    "\tverbose = false,\n",
    ")::Tuple{Vector{Vector{Float64}}, Vector{Vector{Float64}}}\n",
    "\n",
    "\t# check sizes of everything \n",
    "\tnx, nu = size(B)\n",
    "\t@assert size(A) == (nx, nx)\n",
    "\t@assert size(Q) == (nx, nx)\n",
    "\t@assert size(R) == (nu, nu)\n",
    "\t@assert size(Qf) == (nx, nx)\n",
    "\t@assert length(x_ic) == nx\n",
    "\n",
    "\t# TODO: \n",
    "\n",
    "\t# create cvx variables where each column is a time step \n",
    "\t# hint: x_k = X[:,k], u_k = U[:,k]\n",
    "\tX = cvx.Variable(nx, N)\n",
    "\tU = cvx.Variable(nu, N - 1)\n",
    "\n",
    "\t# create cost \n",
    "\t# hint: you can't do x'*Q*x in Convex.jl, you must do cvx.quadform(x,Q)\n",
    "\t# hint: add all of your cost terms to `cost` \n",
    "\tcost = 0\n",
    "\tfor k ∈ 1:(N-1)\n",
    "\n",
    "\t\t# add stagewise cost \n",
    "\t\tcost += 0\n",
    "\tend\n",
    "\n",
    "\t# add terminal cost \n",
    "\tcost += 0\n",
    "\n",
    "\t# initialize cvx problem \n",
    "\tprob = cvx.minimize(cost)\n",
    "\n",
    "\t# TODO: initial condition constraint \n",
    "\t# hint: you can add constraints to our problem like this:\n",
    "\t# prob.constraints += (Gz == h) \n",
    "\n",
    "\tfor k ∈ 1:(N-1)\n",
    "\t\t# dynamics constraints \n",
    "\t\t# prob.constraints += \n",
    "\tend\n",
    "\n",
    "\t# solve problem (silent solver tells us the output)\n",
    "\tcvx.solve!(prob, ECOS.Optimizer; silent_solver = !verbose)\n",
    "\n",
    "\tif prob.status != cvx.MathOptInterface.OPTIMAL\n",
    "\t\terror(\"Convex.jl problem failed to solve for some reason\")\n",
    "\tend\n",
    "\n",
    "\t# convert the solution matrices into vectors of vectors \n",
    "\tX = vec_from_mat(X.value)\n",
    "\tU = vec_from_mat(U.value)\n",
    "\n",
    "\treturn X, U\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a7bb0",
   "metadata": {},
   "source": [
    "Now let's solve this problem for a given initial condition, and simulate it to see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"LQR via Convex.jl\" begin\n",
    "\n",
    "\t# problem setup stuff \n",
    "\tdt = 0.1\n",
    "\ttf = 5.0\n",
    "\tt_vec = 0:dt:tf\n",
    "\tN = length(t_vec)\n",
    "\tA, B = double_integrator_AB(dt)\n",
    "\tnx, nu = size(B)\n",
    "\tQ = diagm(ones(nx))\n",
    "\tR = diagm(ones(nu))\n",
    "\tQf = 5 * Q\n",
    "\n",
    "\t# initial condition \n",
    "\tx_ic = [5, 7, 2, -1.4]\n",
    "\n",
    "\t# setup and solve our convex optimization problem (verbose = true for submission)\n",
    "\tXcvx, Ucvx = convex_trajopt(A, B, Q, R, Qf, N, x_ic; verbose = false)\n",
    "\n",
    "\t# TODO: simulate with the dynamics with control Ucvx, storing the \n",
    "\t# state in Xsim \n",
    "\n",
    "\t# initial condition \n",
    "\tXsim = [zeros(nx) for i ∈ 1:N]\n",
    "\tXsim[1] = 1 * x_ic\n",
    "\n",
    "\t# TODO dynamics simulation \n",
    "\n",
    "\n",
    "\t@test length(Xsim) == N\n",
    "\t@test norm(Xsim[end]) > 1e-13\n",
    "\t#----------------------plotting-----------------------\n",
    "\tXsim_m = mat_from_vec(Xsim)\n",
    "\n",
    "\t# plot state history \n",
    "\tdisplay(plot(t_vec, Xsim_m', label = [\"x₁\" \"x₂\" \"ẋ₁\" \"ẋ₂\"],\n",
    "\t\ttitle = \"State History\",\n",
    "\t\txlabel = \"time (s)\", ylabel = \"x\"))\n",
    "\n",
    "\t# plot trajectory in x1 x2 space \n",
    "\tdisplay(plot(Xsim_m[1, :], Xsim_m[2, :],\n",
    "\t\ttitle = \"Trajectory in State Space\",\n",
    "\t\tylabel = \"x₂\", xlabel = \"x₁\", label = \"\"))\n",
    "\t#----------------------plotting-----------------------\n",
    "\n",
    "\t# tests \n",
    "\t@test 1e-14 < maximum(norm.(Xsim .- Xcvx, Inf)) < 1e-3\n",
    "\t@test isapprox(Ucvx[1], [-7.8532442316767, -4.127120137234], atol = 1e-3)\n",
    "\t@test isapprox(Xcvx[end], [-0.02285990, -0.07140241, -0.21259, -0.1540299], atol = 1e-3)\n",
    "\t@test 1e-14 < norm(Xcvx[end] - Xsim[end]) < 1e-3\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14994e",
   "metadata": {},
   "source": [
    "### Bellman's Principle of Optimality\n",
    "Now we will test Bellman's Principle of optimality. This can be phrased in many different ways, but the main gist is that any section of an optimal trajectory must be optimal. Our original optimization problem was the above problem:\n",
    "$$ \\begin{align} \\min_{x_{1:N},u_{1:N-1}} \\quad & \\sum_{i=1}^{N-1} \\bigg[ \\frac{1}{2} x_i^TQx_i + \\frac{1}{2} u_i^TRu_i \\bigg] + \\frac{1}{2}x_N^TQ_fx_N\\\\ \n",
    " \\text{st} \\quad & x_1 = x_{\\text{IC}} \\\\ \n",
    " & x_{i+1} = A x_i + Bu_i \\quad \\text{for } i = 1,2,\\ldots,N-1 \n",
    " \\end{align}$$\n",
    "which has a solution $x^*_{1:N},u^*_{1:N-1}$. Now let's look at optimizing over a subsection of this trajectory. That means that instead of solving for $x_{1:N},u_{1:N-1}$, we are now solving for $x_{L:N},u_{L:N-1}$ for some new timestep $1 < L < N$. What we are going to do is take the initial condition from $x^*_L$ from our original optimization problem, and setup a new optimization problem that optimizes over $x_{L:N},u_{L:N-1}$:\n",
    "$$ \\begin{align} \\min_{x_{L:N},u_{L:N-1}} \\quad & \\sum_{i=L}^{N-1} \\bigg[ \\frac{1}{2} x_i^TQx_i + \\frac{1}{2} u_i^TRu_i \\bigg] + \\frac{1}{2}x_N^TQ_fx_N\\\\ \n",
    " \\text{st     } \\quad & x_L = x^*_L \\\\ \n",
    " & x_{i+1} = A x_i + Bu_i \\quad \\text{for } i = L,L + 1,\\ldots,N-1 \n",
    " \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9071f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Bellman's Principle of Optimality\" begin\n",
    "\n",
    "\t# problem setup \n",
    "\tdt = 0.1\n",
    "\ttf = 5.0\n",
    "\tt_vec = 0:dt:tf\n",
    "\tN = length(t_vec)\n",
    "\tA, B = double_integrator_AB(dt)\n",
    "\tnx, nu = size(B)\n",
    "\tx0 = [5, 7, 2, -1.4] # initial condition \n",
    "\tQ = diagm(ones(nx))\n",
    "\tR = diagm(ones(nu))\n",
    "\tQf = 5 * Q\n",
    "\n",
    "\t# solve for X_{1:N}, U_{1:N-1} with convex optimization\n",
    "\tXcvx1, Ucvx1 = convex_trajopt(A, B, Q, R, Qf, N, x0; verbose = false)\n",
    "\n",
    "\t# now let's solve a subsection of this trajectory \n",
    "\tL = 18\n",
    "\tN_2 = N - L + 1\n",
    "\n",
    "\t# here is our updated initial condition from the first problem \n",
    "\tx0_2 = Xcvx1[L]\n",
    "\tXcvx2, Ucvx2 = convex_trajopt(A, B, Q, R, Qf, N_2, x0_2; verbose = false)\n",
    "\n",
    "\t# test if these trajectories match for the times they share \n",
    "\tU_error = Ucvx1[L:end] .- Ucvx2\n",
    "\tX_error = Xcvx1[L:end] .- Xcvx2\n",
    "\t@test 1e-14 < maximum(norm.(U_error)) < 1e-3\n",
    "\t@test 1e-14 < maximum(norm.(X_error)) < 1e-3\n",
    "\n",
    "\n",
    "\t# ---------------------------plotting ------------------------------\n",
    "\tX1m = mat_from_vec(Xcvx1)\n",
    "\tX2m = mat_from_vec(Xcvx2)\n",
    "\tplot(X2m[1, :], X2m[2, :], label = \"optimal subtrajectory\", lw = 5, ls = :dot)\n",
    "\tdisplay(plot!(X1m[1, :], X1m[2, :],\n",
    "\t\ttitle = \"Trajectory in State Space\",\n",
    "\t\tylabel = \"x₂\", xlabel = \"x₁\", label = \"full trajectory\"))\n",
    "\t# ---------------------------plotting ------------------------------\n",
    "\n",
    "\t@test isapprox(Xcvx1[end], [-0.02285990, -0.07140241, -0.21259, -0.1540299], rtol = 1e-3)\n",
    "\t@test 1e-14 < norm(Xcvx1[end] - Xcvx2[end], Inf) < 1e-3\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80ae78",
   "metadata": {},
   "source": [
    "## Part C: Finite-Horizon LQR via Riccati (10 pts)\n",
    "Now we are going to solve the original finite-horizon LQR problem:\n",
    "$$ \\begin{align} \\min_{x_{1:N},u_{1:N-1}} \\quad & \\sum_{i=1}^{N-1} \\bigg[ \\frac{1}{2} x_i^TQx_i + \\frac{1}{2} u_i^TRu_i \\bigg] + \\frac{1}{2}x_N^TQ_fx_N\\\\ \n",
    " \\text{st} \\quad & x_1 = x_{\\text{IC}} \\\\ \n",
    " & x_{i+1} = A x_i + Bu_i \\quad \\text{for } i = 1,2,\\ldots,N-1 \n",
    " \\end{align}$$\n",
    " with a Riccati recursion instead of convex optimization. We describe our optimal cost-to-go function (aka the Value function) as the following:\n",
    " \n",
    " $$V_k(x) = \\frac{1}{2}x^TP_kx$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c43c4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fhlqr"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "use the Riccati recursion to calculate the cost to go quadratic matrix P and \n",
    "optimal control gain K at every time step. Return these as a vector of matrices, \n",
    "where P_k = P[k], and K_k = K[k]\n",
    "\"\"\"\n",
    "function fhlqr(A::Matrix, # A matrix \n",
    "\tB::Matrix, # B matrix \n",
    "\tQ::Matrix, # cost weight \n",
    "\tR::Matrix, # cost weight \n",
    "\tQf::Matrix,# term cost weight \n",
    "\tN::Int64,   # horizon size \n",
    ")::Tuple{Vector{Matrix{Float64}}, Vector{Matrix{Float64}}} # return two matrices \n",
    "\n",
    "\t# check sizes of everything \n",
    "\tnx, nu = size(B)\n",
    "\t@assert size(A) == (nx, nx)\n",
    "\t@assert size(Q) == (nx, nx)\n",
    "\t@assert size(R) == (nu, nu)\n",
    "\t@assert size(Qf) == (nx, nx)\n",
    "\n",
    "\t# instantiate S and K \n",
    "\tP = [zeros(nx, nx) for i ∈ 1:N]\n",
    "\tK = [zeros(nu, nx) for i ∈ 1:N-1]\n",
    "\n",
    "\t# initialize S[N] with Qf \n",
    "\tP[N] = deepcopy(Qf)\n",
    "\n",
    "\t# Riccati\n",
    "\tfor k ∈ NaN:NaN\n",
    "\t\t# TODO \n",
    "\n",
    "\tend\n",
    "\n",
    "\treturn P, K\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3304e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Convex trajopt vs LQR\" begin\n",
    "\n",
    "\t# problem stuff\n",
    "\tdt = 0.1\n",
    "\ttf = 5.0\n",
    "\tt_vec = 0:dt:tf\n",
    "\tN = length(t_vec)\n",
    "\tA, B = double_integrator_AB(dt)\n",
    "\tnx, nu = size(B)\n",
    "\tx0 = [5, 7, 2, -1.4] # initial condition \n",
    "\tQ = diagm(ones(nx))\n",
    "\tR = diagm(ones(nu))\n",
    "\tQf = 5 * Q\n",
    "\n",
    "\t# solve for X_{1:N}, U_{1:N-1} with convex optimization\n",
    "\tXcvx, Ucvx = convex_trajopt(A, B, Q, R, Qf, N, x0; verbose = false)\n",
    "\tP, K = fhlqr(A, B, Q, R, Qf, N)\n",
    "\t# now let's simulate using Ucvx \n",
    "\tXsim_cvx = [zeros(nx) for i ∈ 1:N]\n",
    "\tXsim_cvx[1] = 1 * x0\n",
    "\tXsim_lqr = [zeros(nx) for i ∈ 1:N]\n",
    "\tXsim_lqr[1] = 1 * x0\n",
    "\tfor i ∈ 1:N-1\n",
    "\t\t# simulate cvx control \n",
    "\t\tXsim_cvx[i+1] = A * Xsim_cvx[i] + B * Ucvx[i]\n",
    "\n",
    "\t\t# TODO: use your FHLQR control gains K to calculate u_lqr\n",
    "\t\t# simulate lqr control \n",
    "\t\tu_lqr = zeros(2)\n",
    "\t\tXsim_lqr[i+1] = A * Xsim_lqr[i] + B * u_lqr\n",
    "\tend\n",
    "\n",
    "\n",
    "\t@test isapprox(Xsim_lqr[end], [-0.02286201, -0.0714058, -0.21259, -0.154030], rtol = 1e-3)\n",
    "\t@test 1e-13 < norm(Xsim_lqr[end] - Xsim_cvx[end]) < 1e-3\n",
    "\t@test 1e-13 < maximum(norm.(Xsim_lqr - Xsim_cvx)) < 1e-3\n",
    "\n",
    "\n",
    "\t# ------------------------plotting--------------------------\n",
    "\tX1m = mat_from_vec(Xsim_cvx)\n",
    "\tX2m = mat_from_vec(Xsim_lqr)\n",
    "\t# plot trajectory in x1 x2 space \n",
    "\tplot(X1m[1, :], X1m[2, :], label = \"cvx trajectory\", lw = 4, ls = :dot)\n",
    "\tdisplay(plot!(X2m[1, :], X2m[2, :],\n",
    "\t\ttitle = \"Trajectory in State Space\",\n",
    "\t\tylabel = \"x₂\", xlabel = \"x₁\", lw = 2, label = \"lqr trajectory\"))\n",
    "\t# ------------------------plotting--------------------------\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fddcbd3",
   "metadata": {},
   "source": [
    "To emphasize that these two methods for solving the optimization problem result in the same solutions, we are now going to sample initial conditions and run both solutions. You will have to fill in your LQR policy again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random: Random\n",
    "Random.seed!(1)\n",
    "@testset \"Convex trajopt vs LQR\" begin\n",
    "\n",
    "\t# problem stuff\n",
    "\tdt = 0.1\n",
    "\ttf = 5.0\n",
    "\tt_vec = 0:dt:tf\n",
    "\tN = length(t_vec)\n",
    "\tA, B = double_integrator_AB(dt)\n",
    "\tnx, nu = size(B)\n",
    "\tQ = diagm(ones(nx))\n",
    "\tR = diagm(ones(nu))\n",
    "\tQf = 5 * Q\n",
    "\n",
    "\n",
    "\tplot()\n",
    "\tfor ic_iter ∈ 1:20\n",
    "\t\tx0 = [5 * randn(2); 1 * randn(2)]\n",
    "\t\t# solve for X_{1:N}, U_{1:N-1} with convex optimization\n",
    "\t\tXcvx, Ucvx = convex_trajopt(A, B, Q, R, Qf, N, x0; verbose = false)\n",
    "\t\tP, K = fhlqr(A, B, Q, R, Qf, N)\n",
    "\t\tXsim_cvx = [zeros(nx) for i ∈ 1:N]\n",
    "\t\tXsim_cvx[1] = 1 * x0\n",
    "\t\tXsim_lqr = [zeros(nx) for i ∈ 1:N]\n",
    "\t\tXsim_lqr[1] = 1 * x0\n",
    "\t\tfor i ∈ 1:N-1\n",
    "\t\t\t# simulate cvx control \n",
    "\t\t\tXsim_cvx[i+1] = A * Xsim_cvx[i] + B * Ucvx[i]\n",
    "\n",
    "\t\t\t# TODO: use your FHLQR control gains K to calculate u_lqr\n",
    "\t\t\t# simulate lqr control \n",
    "\t\t\tu_lqr = zeros(2)\n",
    "\t\t\tXsim_lqr[i+1] = A * Xsim_lqr[i] + B * u_lqr\n",
    "\t\tend\n",
    "\n",
    "\t\t@test 1e-13 < norm(Xsim_lqr[end] - Xsim_cvx[end]) < 1e-3\n",
    "\t\t@test 1e-13 < maximum(norm.(Xsim_lqr - Xsim_cvx)) < 1e-3\n",
    "\n",
    "\t\t# ------------------------plotting--------------------------\n",
    "\t\tX1m = mat_from_vec(Xsim_cvx)\n",
    "\t\tX2m = mat_from_vec(Xsim_lqr)\n",
    "\t\tplot!(X2m[1, :], X2m[2, :], label = \"\", lw = 4, ls = :dot)\n",
    "\t\tplot!(X1m[1, :], X1m[2, :], label = \"\", lw = 2)\n",
    "\tend\n",
    "\tdisplay(plot!(title = \"cvx vs LQR\", ylabel = \"x₂\", xlabel = \"x₁\"))\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac015c",
   "metadata": {},
   "source": [
    "## Part D: Why LQR is so great (10 pts)\n",
    "\n",
    "Now we are going to emphasize two reasons why the feedback policy from LQR is so useful:\n",
    "\n",
    "1. It is robust to noise and model uncertainty (the Convex approach would require re-solving of the problem every time the new state differs from the expected state (this is MPC, more on this in Q3)\n",
    "2. We can drive to any achievable goal state with $u = -K(x - x_{goal})$\n",
    "\n",
    "First we are going to look at a simulation with the following white noise:\n",
    "$$x_{k+1} = Ax_k + Bu_k + \\text{noise}$$\n",
    "Where noise $ \\sim \\mathcal{N}(0,\\Sigma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee377725",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Why LQR is great reason 1\" begin\n",
    "\n",
    "\t# problem stuff\n",
    "\tdt = 0.1\n",
    "\ttf = 7.0\n",
    "\tt_vec = 0:dt:tf\n",
    "\tN = length(t_vec)\n",
    "\tA, B = double_integrator_AB(dt)\n",
    "\tnx, nu = size(B)\n",
    "\tx0 = [5, 7, 2, -1.4] # initial condition \n",
    "\tQ = diagm(ones(nx))\n",
    "\tR = diagm(ones(nu))\n",
    "\tQf = 10 * Q\n",
    "\n",
    "\t# solve for X_{1:N}, U_{1:N-1} with convex optimization\n",
    "\tXcvx, Ucvx = convex_trajopt(A, B, Q, R, Qf, N, x0; verbose = false)\n",
    "\tP, K = fhlqr(A, B, Q, R, Qf, N)\n",
    "\t# now let's simulate using Ucvx \n",
    "\tXsim_cvx = [zeros(nx) for i ∈ 1:N]\n",
    "\tXsim_cvx[1] = 1 * x0\n",
    "\tXsim_lqr = [zeros(nx) for i ∈ 1:N]\n",
    "\tXsim_lqr[1] = 1 * x0\n",
    "\tfor i ∈ 1:N-1\n",
    "\t\t# sampled noise to be added after each step \n",
    "\t\tnoise = [0.005 * randn(2); 0.1 * randn(2)]\n",
    "\n",
    "\t\t# simulate cvx control \n",
    "\t\tXsim_cvx[i+1] = A * Xsim_cvx[i] + B * Ucvx[i] + noise\n",
    "\n",
    "\t\t# TODO: use your FHLQR control gains K to calculate u_lqr\n",
    "\t\t# simulate lqr control \n",
    "\t\tu_lqr = zeros(2)\n",
    "\t\tXsim_lqr[i+1] = A * Xsim_lqr[i] + B * u_lqr + noise\n",
    "\tend\n",
    "\n",
    "\t# make sure our LQR achieved the goal \n",
    "\t@test norm(Xsim_cvx[end]) > norm(Xsim_lqr[end])\n",
    "\t@test norm(Xsim_lqr[end]) < 0.7\n",
    "\t@test norm(Xsim_cvx[end]) > 2.0\n",
    "\n",
    "\n",
    "\t# ------------------------plotting--------------------------\n",
    "\tX1m = mat_from_vec(Xsim_cvx)\n",
    "\tX2m = mat_from_vec(Xsim_lqr)\n",
    "\t# plot trajectory in x1 x2 space \n",
    "\tplot(X1m[1, :], X1m[2, :], label = \"CVX Trajectory (no replanning)\", lw = 4, ls = :dot)\n",
    "\tdisplay(plot!(X2m[1, :], X2m[2, :],\n",
    "\t\ttitle = \"Trajectory in State Space (Noisy Dynamics)\",\n",
    "\t\tylabel = \"x₂\", xlabel = \"x₁\", lw = 2, label = \"LQR Trajectory\"))\n",
    "\tecvx = [norm(x[1:2]) for x in Xsim_cvx]\n",
    "\telqr = [norm(x[1:2]) for x in Xsim_lqr]\n",
    "\tplot(t_vec, elqr, label = \"LQR Trajectory\", ylabel = \"|x - xgoal|\",\n",
    "\t\txlabel = \"time (s)\", title = \"Error for CVX vs LQR (Noisy Dynamics)\")\n",
    "\tdisplay(plot!(t_vec, ecvx, label = \"CVX Trajectory (no replanning)\"))\n",
    "\t# ------------------------plotting--------------------------\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8bd00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@testset \"Why LQR is great reason 2\" begin\n",
    "\n",
    "\t# problem stuff\n",
    "\tdt = 0.1\n",
    "\ttf = 20.0\n",
    "\tt_vec = 0:dt:tf\n",
    "\tN = length(t_vec)\n",
    "\tA, B = double_integrator_AB(dt)\n",
    "\tnx, nu = size(B)\n",
    "\tx0 = [5, 7, 2, -1.4] # initial condition \n",
    "\tQ = diagm(ones(nx))\n",
    "\tR = diagm(ones(nu))\n",
    "\tQf = 10 * Q\n",
    "\n",
    "\tP, K = fhlqr(A, B, Q, R, Qf, N)\n",
    "\n",
    "\t# TODO: specify a goal state with 0 velocity within a 5m radius of 0 \n",
    "\txgoal = [-3.5, -3.5, 0, 0]\n",
    "\t@test norm(xgoal[1:2]) < 5\n",
    "\t@test norm(xgoal[3:4]) < 1e-13 # ensure 0 velocity\n",
    "\n",
    "\tXsim_lqr = [zeros(nx) for i ∈ 1:N]\n",
    "\tXsim_lqr[1] = 1 * x0\n",
    "\n",
    "\tfor i ∈ 1:N-1\n",
    "\t\t# TODO: use your FHLQR control gains K to calculate u_lqr\n",
    "\t\t# simulate lqr control \n",
    "\t\tu_lqr = zeros(2)\n",
    "\t\tXsim_lqr[i+1] = A * Xsim_lqr[i] + B * u_lqr\n",
    "\tend\n",
    "\n",
    "\t@test norm(Xsim_lqr[end][1:2] - xgoal[1:2]) < 0.1\n",
    "\n",
    "\t# ------------------------plotting--------------------------\n",
    "\tXm = mat_from_vec(Xsim_lqr)\n",
    "\tplot(xgoal[1:1], xgoal[2:2], seriestype = :scatter, label = \"goal state\")\n",
    "\tdisplay(plot!(Xm[1, :], Xm[2, :],\n",
    "\t\ttitle = \"Trajectory in State Space\",\n",
    "\t\tylabel = \"x₂\", xlabel = \"x₁\", lw = 2, label = \"LQR Trajectory\"))\n",
    "\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3329e",
   "metadata": {},
   "source": [
    "## Part E: Infinite-horizon LQR (10 pts)\n",
    "Up until this point, we have looked at finite-horizon LQR which only considers a finite number of timesteps in our trajectory.  When this problem is solved with a Riccati recursion, there is a new feedback gain matrix $K_k$ for each timestep. As the length of the trajectory increases, the first feedback gain matrix $K_1$ will begin to converge on what we call the \"infinite-horizon LQR gain\".  This is the value that $K_1$ converges to as $N \\rightarrow \\infty$.\n",
    "\n",
    "Below, we will plot the values of $P$ and $K$ throughout the horizon and observe this convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# half vectorization of a matrix \n",
    "function vech(A)\n",
    "\treturn A[tril(trues(size(A)))]\n",
    "end\n",
    "@testset \"P and K time analysis\" begin\n",
    "\n",
    "\t# problem stuff\n",
    "\tdt = 0.1\n",
    "\ttf = 10.0\n",
    "\tt_vec = 0:dt:tf\n",
    "\tN = length(t_vec)\n",
    "\tA, B = double_integrator_AB(dt)\n",
    "\tnx, nu = size(B)\n",
    "\n",
    "\t# cost terms \n",
    "\tQ = diagm(ones(nx))\n",
    "\tR = 0.5 * diagm(ones(nu))\n",
    "\tQf = randn(nx, nx)\n",
    "\tQf = Qf' * Qf + I\n",
    "\n",
    "\tP, K = fhlqr(A, B, Q, R, Qf, N)\n",
    "\n",
    "\tPm = hcat(vech.(P)...)\n",
    "\tKm = hcat(vec.(K)...)\n",
    "\n",
    "\t# make sure these things converged \n",
    "\t@test 1e-13 < norm(P[1] - P[2]) < 1e-3\n",
    "\t@test 1e-13 < norm(K[1] - K[2]) < 1e-3\n",
    "\n",
    "\tdisplay(plot(t_vec, Pm', label = \"\", title = \"Cost-to-go Matrix (P)\", xlabel = \"time(s)\"))\n",
    "\tdisplay(plot(t_vec[1:end-1], Km', label = \"\", title = \"Gain Matrix (K)\", xlabel = \"time(s)\"))\n",
    "\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a23f92",
   "metadata": {},
   "source": [
    "Complete this infinite horizon LQR function where you do a Riccati recursion until the cost to go matrix P converges:\n",
    "\n",
    "$$ \\|P_k - P_{k+1}\\| \\leq \\text{tol}$$\n",
    "\n",
    "And return the steady state $P$ and $K$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b01788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "P,K = ihlqr(A,B,Q,R)\n",
    "\n",
    "TODO: complete this infinite horizon LQR function where \n",
    "you do the Riccati recursion until the cost to go matrix \n",
    "P converges to a steady value |P_k - P_{k+1}| ≤ tol \n",
    "\"\"\"\n",
    "function ihlqr(A::Matrix,       # vector of A matrices \n",
    "\tB::Matrix,       # vector of B matrices\n",
    "\tQ::Matrix,       # cost matrix Q \n",
    "\tR::Matrix;       # cost matrix R \n",
    "\tmax_iter = 1000, # max iterations for Riccati\n",
    "\ttol = 1e-5,       # convergence tolerance\n",
    ")::Tuple{Matrix, Matrix} # return two matrices \n",
    "\n",
    "\t# get size of x and u from B \n",
    "\tnx, nu = size(B)\n",
    "\n",
    "\t# initialize S with Q\n",
    "\tP = deepcopy(Q)\n",
    "\n",
    "\t# Riccati \n",
    "\tfor riccati_iter ∈ 1:max_iter\n",
    "\n",
    "\t\t# TODO \n",
    "\n",
    "\tend\n",
    "\terror(\"ihlqr did not converge\")\n",
    "end\n",
    "@testset \"ihlqr test\" begin\n",
    "\t# problem stuff\n",
    "\tdt = 0.1\n",
    "\tA, B = double_integrator_AB(dt)\n",
    "\tnx, nu = size(B)\n",
    "\n",
    "\t# we're just going to modify the system a little bit \n",
    "\t# so the following graphs are still interesting\n",
    "\n",
    "\tQ = diagm(ones(nx))\n",
    "\tR = 0.5 * diagm(ones(nu))\n",
    "\tP, K = ihlqr(A, B, Q, R)\n",
    "\n",
    "\t# check this P is in fact a solution to the Riccati equation\n",
    "\t@test typeof(P) == Matrix{Float64}\n",
    "\t@test typeof(K) == Matrix{Float64}\n",
    "\t@test 1e-13 < norm(Q + K' * R * K + (A - B * K)'P * (A - B * K) - P) < 1e-3\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43bee19",
   "metadata": {},
   "source": [
    "## Part F (5 pts): One sentence short answer\n",
    "\n",
    "1. What is the difference between stage cost and terminal cost?\n",
    "\n",
    "**put one sentence answer here**\n",
    "\n",
    "2. What is a terminal cost trying to capture? (think about dynamic programming)\n",
    "\n",
    "**put one sentence answer here**\n",
    "\n",
    "3. In order to build an LQR controller for a linear system, do we need to know the initial state $x_0$?\n",
    "\n",
    "**put one sentence answer here**\n",
    "\n",
    "4. If a linear system is uncontrollable, will the finite-horizon LQR convex optimization problem have a solution?\n",
    "\n",
    "**put one sentence answer here**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
